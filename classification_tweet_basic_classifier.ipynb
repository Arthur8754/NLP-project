{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/raphaelubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2023-03-17 23:30:51.774284: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-17 23:30:54.156734: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-17 23:30:54.156940: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-17 23:30:54.156951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-17 23:30:55.771766: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-17 23:30:55.772036: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-17 23:30:55.772091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (raphaelubuntu-HP-ProBook-450-G7): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "import pandas as pd# Scale the data\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler# Pipeline, Gridsearch, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV# Plot the confusion matrix at the end of the tutorial\n",
    "#from sklearn.metrics import plot_confusion_matrix# Classification Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import _stop_words as stop_words\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "from emot.emo_unicode import UNICODE_EMOJI # For emojis\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "#import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import confusion_matrix \n",
    "#from sklearn.metrics import classification_report\n",
    "#from sklearn import metrics\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recuperation du dataset\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression des colonnes non-utilisées\n",
    "df = df.drop(['id','keyword','location'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création des différentes fonction de pré-traitements\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "stopwords = stop_words.ENGLISH_STOP_WORDS\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#converting emojis\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "#removing html code\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "#removing urls\n",
    "def remove_urls(text):\n",
    "    pattern = re.compile(r'https?://(www\\.)?(\\w+)(\\.\\w+)(/\\w*)?')\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    return text\n",
    "\n",
    "#removing mentions\n",
    "def remove_mentions(text):\n",
    "    pattern = re.compile(r\"@\\w+\")\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    return text\n",
    "\n",
    "#removing punctuations\n",
    "def remove_punctuations(text):\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), \" \",text)\n",
    "    return text\n",
    "\n",
    "#Clean global with tokenizer and lemmatizer\n",
    "def clean(doc):\n",
    "    text_no_namedentities = []\n",
    "    document = nlp(doc)\n",
    "    ents = [e.text for e in document.ents]\n",
    "    for item in document:\n",
    "        if item.text in ents:\n",
    "            pass\n",
    "        else:\n",
    "            text_no_namedentities.append(item.text)\n",
    "    doc = (\" \".join(text_no_namedentities))\n",
    "\n",
    "    doc = doc.lower().strip()\n",
    "    doc = doc.replace(\"</br>\", \" \") \n",
    "    doc = doc.replace(\"-\", \" \") \n",
    "    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n",
    "    doc = \" \".join([token for token in doc.split() if token not in stopwords])    \n",
    "    doc = \"\".join([lemmatizer.lemmatize(word) for word in doc])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application des pré-traitements\n",
    "df['text'] = df['text'].apply(convert_emojis)\n",
    "df['text'] = df['text'].apply(remove_html)\n",
    "df['text'] = df['text'].apply(remove_urls)\n",
    "df['text'] = df['text'].apply(remove_mentions)\n",
    "df['text'] = df['text'].apply(remove_punctuations)\n",
    "\n",
    "df['text'] = df['text'].apply(clean)\n",
    "df = df.dropna().drop_duplicates()\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(df['text'])\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, max_features = 20000,ngram_range=(1,2)) \n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(docs)\n",
    "docs = tfidf_vectorizer_vectors.toarray()\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "bow = cv.fit_transform(df['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['target']\n",
    "X = df['text']\n",
    "\n",
    "X_tfidf = docs\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf,y,random_state=42)\n",
    "\n",
    "X_bow = bow\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow,y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Priority column acts as a target variable and other columns as predictors\n",
    "CountVectorizedData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "CountVectorizedData['Priority']=df['target']\n",
    "\n",
    "WordsVocab=CountVectorizedData.columns[:-1]\n",
    "\n",
    "def FunctionText2Vec(inpTextData):\n",
    "    # Converting the text to numeric data\n",
    "    X = vectorizer.transform(inpTextData)\n",
    "    CountVecData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    # Creating empty dataframe to hold sentences\n",
    "    W2Vec_Data=pd.DataFrame()\n",
    "    \n",
    "    # Looping through each row for the data\n",
    "    for i in range(CountVecData.shape[0]):\n",
    "\n",
    "        # initiating a sentence with all zeros\n",
    "        Sentence = np.zeros(300)\n",
    "\n",
    "        # Looping thru each word in the sentence and if its present in \n",
    "        # the Word2Vec model then storing its vector\n",
    "        for word in WordsVocab[CountVecData.iloc[i , :]>=1]:\n",
    "            #print(word)\n",
    "            if word in w2v_model.key_to_index.keys():    \n",
    "                Sentence=Sentence+w2v_model[word]\n",
    "        # Appending the sentence to the dataframe\n",
    "        W2Vec_Data=W2Vec_Data.append(pd.DataFrame([Sentence]))\n",
    "    return(W2Vec_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2Vec_Data=FunctionText2Vec(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2Vec_Data.reset_index(inplace=True, drop=True)\n",
    "W2Vec_Data['target']=CountVectorizedData['Priority']\n",
    "\n",
    "TargetVariable=W2Vec_Data.columns[-1]\n",
    "Predictors=W2Vec_Data.columns[:-1]\n",
    " \n",
    "X=W2Vec_Data[Predictors].values\n",
    "y=W2Vec_Data[TargetVariable].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Choose either standardization or Normalization\n",
    "# On this data Min Max Normalization is used because we need to fit Naive Bayes\n",
    "\n",
    "# Choose between standardization and MinMAx normalization\n",
    "mm=MinMaxScaler()\n",
    "mm=mm.fit(X)\n",
    "X=mm.transform(X)\n",
    "\n",
    "# Split the data into training and testing set\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X, y, test_size=0.3, random_state=428)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =  LogisticRegression(random_state=42)\n",
    "dt =  DecisionTreeClassifier(random_state=42)\n",
    "rf =  RandomForestClassifier(random_state=42)\n",
    "knn = KNeighborsClassifier()\n",
    "svm = svm.SVC(random_state=42)\n",
    "mnb = MultinomialNB()\n",
    "gnb = GaussianNB()\n",
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [dt, rf, knn, lr, svm, mnb, bnb, gnb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training for Bag of Words data\")\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for classifier in grids:\n",
    "    st = time.time()\n",
    "    classifier.fit(X_train_bow,y_train_bow)\n",
    "    et = time.time()\n",
    "    train_pred = classifier.predict(X_train_bow)\n",
    "    pred = classifier.predict(X_test_bow)\n",
    "    train_results.append(train_pred)\n",
    "    test_results.append(pred)\n",
    "    print(\"Training F1 Score:\")\n",
    "    print(classifier.__class__.__name__, f1_score(y_train_bow, train_pred))\n",
    "    print(\"Testing F1 Score:\")\n",
    "    print(classifier.__class__.__name__, f1_score(y_test_bow, pred))\n",
    "    print(\"Fit time:\")\n",
    "    print(classifier.__class__.__name__,et-st)\n",
    "    print(\"----------\")\n",
    "\n",
    "train_result = 0\n",
    "test_result = 0\n",
    "\n",
    "for result in train_results:\n",
    "    train_result += result\n",
    "\n",
    "for result in test_results:\n",
    "    test_result += result\n",
    "\n",
    "train_result = [int(x) for x in np.around(train_result/len(grids))]\n",
    "test_result = [int(x) for x in np.around(test_result/len(grids))]\n",
    "\n",
    "print(\"Training F1 Score:\")\n",
    "print(\"Voting Classifier: \", f1_score(y_train_bow, train_result))\n",
    "print(\"Testing F1 Score:\")\n",
    "print(\"Voting Classifier :\", f1_score(y_test_bow, test_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training for TFIDF data\")\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for classifier in grids:\n",
    "    st = time.time()\n",
    "    classifier.fit(X_train_tfidf,y_train_tfidf)\n",
    "    et = time.time()\n",
    "    train_pred = classifier.predict(X_train_tfidf)\n",
    "    pred = classifier.predict(X_test_tfidf)\n",
    "    train_results.append(train_pred)\n",
    "    test_results.append(pred)\n",
    "    print(\"Training F1 Score:\")\n",
    "    print(classifier.__class__.__name__, f1_score(y_train_tfidf, train_pred))\n",
    "    print(\"Testing F1 Score:\")\n",
    "    print(classifier.__class__.__name__, f1_score(y_test_tfidf, pred))\n",
    "    print(\"Fit time:\")\n",
    "    print(classifier.__class__.__name__,et-st)\n",
    "    print(\"----------\")\n",
    "\n",
    "train_result = 0\n",
    "test_result = 0\n",
    "\n",
    "for result in train_results:\n",
    "    train_result += result\n",
    "\n",
    "for result in test_results:\n",
    "    test_result += result\n",
    "\n",
    "train_result = [int(x) for x in np.around(train_result/len(grids))]\n",
    "test_result = [int(x) for x in np.around(test_result/len(grids))]\n",
    "\n",
    "print(\"Training F1 Score:\")\n",
    "print(\"Voting Classifier: \", f1_score(y_train_bow, train_result))\n",
    "print(\"Testing F1 Score:\")\n",
    "print(\"Voting Classifier :\", f1_score(y_test_bow, test_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score:\n",
      "DecisionTreeClassifier 0.9860537190082644\n",
      "Testing F1 Score:\n",
      "DecisionTreeClassifier 0.5641323273360419\n",
      "Fit time:\n",
      "DecisionTreeClassifier 3.1130762100219727\n",
      "----------\n",
      "Training F1 Score:\n",
      "RandomForestClassifier 0.9861751152073732\n",
      "Testing F1 Score:\n",
      "RandomForestClassifier 0.6732804232804234\n",
      "Fit time:\n",
      "RandomForestClassifier 9.2255117893219\n",
      "----------\n",
      "Training F1 Score:\n",
      "KNeighborsClassifier 0.7576537911301859\n",
      "Testing F1 Score:\n",
      "KNeighborsClassifier 0.6415868673050615\n",
      "Fit time:\n",
      "KNeighborsClassifier 0.002583742141723633\n",
      "----------\n",
      "Training F1 Score:\n",
      "LogisticRegression 0.7437241379310344\n",
      "Testing F1 Score:\n",
      "LogisticRegression 0.7203608247422681\n",
      "Fit time:\n",
      "LogisticRegression 0.5158605575561523\n",
      "----------\n",
      "Training F1 Score:\n",
      "SVC 0.8276054763900531\n",
      "Testing F1 Score:\n",
      "SVC 0.7193211488250653\n",
      "Fit time:\n",
      "SVC 4.290931701660156\n",
      "----------\n",
      "Training F1 Score:\n",
      "MultinomialNB 0.4773755656108598\n",
      "Testing F1 Score:\n",
      "MultinomialNB 0.4746059544658494\n",
      "Fit time:\n",
      "MultinomialNB 0.06886744499206543\n",
      "----------\n",
      "Training F1 Score:\n",
      "BernoulliNB 0.06502463054187192\n",
      "Testing F1 Score:\n",
      "BernoulliNB 0.01388888888888889\n",
      "Fit time:\n",
      "BernoulliNB 0.07851004600524902\n",
      "----------\n",
      "Training F1 Score:\n",
      "GaussianNB 0.6533087266016346\n",
      "Testing F1 Score:\n",
      "GaussianNB 0.650210716435882\n",
      "Fit time:\n",
      "GaussianNB 0.027765750885009766\n",
      "----------\n",
      "Training F1 Score:\n",
      "Voting Classifier:  0.7917425622343656\n",
      "Testing F1 Score:\n",
      "Voting Classifier : 0.6515040352164343\n"
     ]
    }
   ],
   "source": [
    "print(\"Training for Word2Vec data\")\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for classifier in grids:\n",
    "    st = time.time()\n",
    "    classifier.fit(X_train_w2v,y_train_w2v)\n",
    "    et = time.time()\n",
    "    train_pred = classifier.predict(X_train_w2v)\n",
    "    pred = classifier.predict(X_test_w2v)\n",
    "    train_results.append(train_pred)\n",
    "    test_results.append(pred)\n",
    "    print(\"Training F1 Score:\")\n",
    "    print(classifier.__class__.__name__, f1_score(y_train_w2v, train_pred))\n",
    "    print(\"Testing F1 Score:\")\n",
    "    print(classifier.__class__.__name__, f1_score(y_test_w2v, pred))\n",
    "    print(\"Fit time:\")\n",
    "    print(classifier.__class__.__name__,et-st)\n",
    "    print(\"----------\")\n",
    "\n",
    "train_result = 0\n",
    "test_result = 0\n",
    "\n",
    "for result in train_results:\n",
    "    train_result += result\n",
    "\n",
    "for result in test_results:\n",
    "    test_result += result\n",
    "\n",
    "train_result = [int(x) for x in np.around(train_result/len(grids))]\n",
    "test_result = [int(x) for x in np.around(test_result/len(grids))]\n",
    "\n",
    "print(\"Training F1 Score:\")\n",
    "print(\"Voting Classifier: \", f1_score(y_train_w2v, train_result))\n",
    "print(\"Testing F1 Score:\")\n",
    "print(\"Voting Classifier :\", f1_score(y_test_w2v, test_result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
